# -*- coding: utf-8 -*-
"""Classification Model (cleaned).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L2n8erGL4j083fg9ag6JJh2ZdUqrsKeS
"""

#importing required libraries
import cv2
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import random
import imutils
from sklearn import svm

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import utils
from keras.models import load_model

#loading the images into the notebook
from google.colab import files
files.upload()

image_list = []
image_cat = []
categories = ["VanGohg" , "Not"]
for i in range(1,34):
  #looping through images and converting to jpg
    image=Image.open(str(i)+'.jpg')
    #resizing the image
    image=image.resize((400, 400))
    #appending to the list
    image_list.append(np.asarray(image))
    cat_num = categories.index(categories[0]) 
    image_cat.append(cat_num)

for i in range(34,67):
  #looping through images and converting to jpg
    image=Image.open(str(i)+'.jpg')
    #resizing the image
    image=image.resize((400, 400))
    #appending to the list
    image_list.append(np.asarray(image))
    cat_num = categories.index(categories[1]) 
    image_cat.append(cat_num)

#appending to array
images = np.array(image_list)
image_cat = np.array(image_cat)

#extracting features
kp_img=[]
for i in range(66):
  #grayscaling images
  grayimg = cv2.cvtColor(images[i], cv2.COLOR_BGR2GRAY)
  #initiating ORB detector
  orb = cv2.ORB_create(nfeatures=1000)
  #finding keypoints and detectors for both images
  kpt, desc = orb.detectAndCompute(grayimg,None)

  img = cv2.drawKeypoints(grayimg, kpt, None)
  kp_img.append(np.asarray(img))

features = np.array(kp_img)

#making training dataset
X = features
y = image_cat

#normalizing the dataset
X = utils.normalize(X, axis=1)

#splitting the data into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)

"""#Building Model"""

#building a seq2seq model
model = Sequential()

model.add(Conv2D(32, kernel_size=(3,3),activation='relu', input_shape=(400,400,3)))
model.add(Conv2D(filters=32,kernel_size=(3,3),padding='Same',activation='relu', kernel_initializer = 'he_uniform'))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Conv2D(filters=64,kernel_size=(3,3),padding='Same',activation='relu', kernel_initializer = 'he_uniform'))
model.add(Conv2D(filters=64,kernel_size=(3,3),padding='Same',activation='relu', kernel_initializer = 'he_uniform'))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Conv2D(filters=128,kernel_size=(3,3),padding='Same',activation='relu', kernel_initializer = 'he_uniform'))
model.add(Conv2D(filters=128,kernel_size=(3,3),padding='Same',activation='relu', kernel_initializer = 'he_uniform'))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Conv2D(filters=256,kernel_size=(3,3),padding='Same',activation='relu', kernel_initializer = 'he_uniform'))
model.add(Conv2D(filters=256,kernel_size=(3,3),padding='Same',activation='relu', kernel_initializer = 'he_uniform'))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

model.add(Flatten())
model.add(Dense(128, activation='relu', kernel_initializer = 'he_uniform'))
model.add(Dense(128, activation='relu', kernel_initializer = 'he_uniform'))
model.add(Dense(1, activation='sigmoid'))

#defining the batch sizes and epochs
batch_size = 30
epochs = 40
opt = SGD()

early_stop = EarlyStopping(monitor='val_loss',patience=50)

model.compile(optimizer=opt, loss="binary_crossentropy", metrics=["accuracy"])

model.summary()

#training the model
history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, 
                    verbose=1, validation_data=(X_test, y_test), callbacks = [early_stop])

#evaluating model over test dataset
model.evaluate(X_test, y_test)

model.save("paintings.h5")

"""#Prediction"""

pred = model.predict(X_test)
classes_x=np.argmax(pred,axis=1)
preds_img = [X_test,classes_x,y_test]
for i in range(7):
  print("Image index:", i, "Predicted label:", preds_img[1][i], "Original label:", preds_img[2][i])